{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksempel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konverter tweets til gensim corpus\n",
    "tweets_tokens = list(tweets_df['tokens'])\n",
    "id2token = corpora.Dictionary(tweets_tokens) # integer id per word\n",
    "tweets_corpus = [id2token.doc2bow(tokens) for tokens in tweets_tokens] # bag-of-word(bow) tuples for hver text - (token-id, optælling)\n",
    "\n",
    "# Bygger LDA model\n",
    "lda_model = LdaModel(corpus=tweets_corpus,  # Corpus af tekster\n",
    "                    id2word=id2token,       # Dictionary af ord-id mapping\n",
    "                     num_topics=10,          # Antal topics\n",
    "                     random_state=142)       # Sætter seed - sikrer samme resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viser mest sandsynlige ord per topic\n",
    "pprint(lda_model.show_topics(formatted=False)) # Viser de 10 mest sandsynlige ord per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduktion til topic modelling med latent dirichlet allocation (LDA)\n",
    "\n",
    "Indtil videre har vi kigget på teknikker, der hjælper os til at få et overblik over ordene, som indgår i data. Disse teknikker egner sig godt til det, som man kalder \"nøgleordsanalyse\"; altså en analyse af de mest centrale ord/termer i et corpus.\n",
    "\n",
    "Vi kan dog også inddrage teknikker til at hjælpe os identificere umiddelbare mønstre og temaer i en samling af tekster.\n",
    "\n",
    "\"Latent Dirichlet Allocaltion\" (LDA) ([Blei, David M. 2012](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)): - en af de mere populære \"topic models\" - er en måde at anvende usuperviseret maskinlæring til at finde umiddelbare temaer i en tekst.\n",
    "\n",
    "**Fordele**\n",
    "- Giver overblik over umiddelbare tematiske mønstre i data\n",
    "- Hjælper til at inddele og kategorisere tekstdata\n",
    "\n",
    "**Ulemper**\n",
    "- Fremgangsmåden præget af vilkårlighed (hvor mange topics giver mening at finde?)\n",
    "- Sårbar over for små ændringer i datahåndtering (hvorvidt man bruger lemmatizer, hvilke stopord man frasorterer)\n",
    "- Ringe ekstern validitet: Modellen siger noget om den pågældende tekstsamling, men kan i ringe grade anvendes på andre tekstdata\n",
    "\n",
    "\n",
    "## Hvordan fungerer topic modelling (LDA)?\n",
    "\n",
    "LDA er en såkaldt \"bag-of-words\" model. Det vil først og fremmest sige, at den ikke tager højde for ordets sætningskontekst, men blot behandler en tekst som en samling af enkeltord.\n",
    "\n",
    "![bow](.././img/bow.jpeg)\n",
    "\n",
    "*[ProgrammerSought.com 2021](https://www.programmersought.com/article/4304366575/)*\n",
    "\n",
    "### LDA kort fortal\n",
    "\n",
    "![lda](.././img/lda.png)\n",
    "\n",
    "- Det antages, at der eksisterer et vis antal \"topics\", der går igen på tværs af alle dokumenter i et corpus\n",
    "- Et \"topic\" er en sandsynlighedsfordeling over en række ord; en samling af ord behæftet med en hvis sandsynlighed\n",
    "- Hvert dokument antages at være sammensat af de forskellige topics; hvert topic er behæftet med en hvis sandsynlighed for at optræde i dokumentet\n",
    "- Ordene i et dokument antages at være \"trukket\" af de forskellige topics\n",
    "\n",
    "Lad os fx antage, at reddit kommentarer består af tre topics: konspiration, forretning og politik. Antagelsen i LDA er så, at en reddit kommentar, som nedenstående, er dannet ud fra en fordeling af disse topics:\n",
    "\n",
    "> Der er en ikke helt ude konspirations teori der bygger på at de store firmaer støttede det så de slap for at blive beskattet, ved at distrahere venstrefløjen.\n",
    "\n",
    "Lad os antage, at en topic model betragter denne kommentar som værende 40% konspiration, 25% forretning og 35% politik. Ordene i kommentaren antages at være taget fra denne fordeling af topics. I hvert topic er visse ord mere sandsynlige end andre (\"konspiration\" mere sandynsligt i konspirations-topic end forretning-topic, \"venstrefløjen\" mere sandsynligt i politik-topic end konspirations-topic, osv.).\n",
    "\n",
    "\n",
    "### LDA som en \"generativ probabilitisk model\"\n",
    "\n",
    "LDA er en \"generativ probabilitisk model\". Det vil sige, at data antages at blive dannet af en generativ proces over en sandsynlighedsfordeling, som er sammensat af *både* observerede og uobserverede variable ([Blei, David M. 2012](http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf)).\n",
    "\n",
    "De observerede variable for LDA er ordene i teksterne, mens de uoverserverede variable er de topics, som dokumenterne antages at være dannet af.\n",
    "\n",
    "Sandsynlighedsfordelingen over ord og topics i en samling dokumenter noteres som følgende:\n",
    "\n",
    "$$\n",
    "p(\\beta_{1:K}, \\theta_{1:D}, z_{1:D}, w_{1:d})\n",
    "$$\n",
    "$$\n",
    "= \\prod_{i=1}^{K}p(\\beta_i)\\prod_{d=1}^{D}p(\\theta_d)\\Big(\\prod_{n=1}^{N}p(z_{d,n}|\\theta_d)p(w_{d,n}|\\beta_{1:K},z_{d,n})\\Big)\n",
    "$$\n",
    "\n",
    "- $\\beta_{1:K}$ er topics,  hvor hvert $\\beta_{k}$ er en sandsynlighedsfordeling af ord\n",
    "- $\\theta_{d,k}$ er topic-andel for topic $k$ for tekst $d$\n",
    "- $z_{d,n}$ er tildeling af topic for $n$ ord i tekst $d$\n",
    "- $w_{d,n}$ er observerede ord $n$ for dokument $d$\n",
    "\n",
    "\n",
    "Da topic-strukturen er *uobserveret* variabel, kan modellen i sig selv ikke give svar på, hvilke emner, som der findes. Vi kan dog med modellen definere topic-strukturen ved at specificere et antal topics, hvorefter modellen så kan foretage beregningen for ords sandsynligheder i de enkelte topics og sandsynlighederne for topics i dokumenterne.\n",
    "\n",
    "Der findes dog forskellige teknikker, der forsøger at løse problemet om at finde den optimale eller mest passende topic struktur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brug af topic models i Python\n",
    "\n",
    "I det følgende ser vi på, hvordan vi kan lave en topic model. Vi bruger pakken [`gensim`](https://radimrehurek.com/gensim/) til formålet.\n",
    "\n",
    "`gensim` er en meget udbredt pakke til topic modelling. Dog er den ikke udviklet til at være kompatibel med `pandas`, hvorfor vi er nødt til at konvertere data til at være kompatibel med `gensim`.\n",
    "\n",
    "Den datastruktur, som `gensim` skal bruge, er et \"gensim corpus\". Et \"gensim corpus\" er en liste af token-optællinger for hver tekst i et corpus. Sådan et corpus dannes ved først at konvertere hvert token i corpus til et numerisk id (hvert token i corpus får unikt numerisk id). Derefter tælles antallet af tokens per tekst, og samles i tuples bestående af det numeriske id og antal hændelser i teksten.\n",
    "\n",
    "### Fra tekster til gensim corpus\n",
    "\n",
    "Til at starte med, skal vi bruge lister af tokens for hver tekst. Disse har vi allerede i vores dataframe (den, som ikke er konverteret til tidy). Vi kan derfor tage disse lister af tokens ud og arbejde med dem for sig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "\n",
    "# Lagr tweet tokens i liste for sig\n",
    "tweets_tokens = list(tweets_df['tokens'])\n",
    "\n",
    "print(tweets_tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derefter kan vi bruge `corpora.Dictionary` funktionen fra `gensim` til at konvertere tokens til numeriske id'er."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Lav en dictionary - integer id per token\n",
    "id2token = corpora.Dictionary(tweets_tokens) # integer id per word\n",
    "\n",
    "print(\"\\n\")\n",
    "print(id2token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id2token` er nu en `gensim.corpora.Dictionary`. Denne indeholder forskellige metoder til at konvertere tekstdata. Blandt andet indeholder den metoden `.doc2bow()`, som danner en \"term-document frequecy\" (optælling af ord i teksten). (\"bow\" er kort for \"bag-of-words\", da optællingen netop ikke tager højde for rækkefølgen).\n",
    "\n",
    "Resultatet er en liste af tuples, hvor hvert tuple består af optælling af hvert token i teksten. \n",
    "\n",
    "Herunder bruges en \"list comprehension\" (et forsimplet for loop, der returnerer en liste), til at konvertere alle tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danner gensim corpus - optælling af tokens per tekst\n",
    "tweets_corpus = [id2token.doc2bow(tokens) for tokens in tweets_tokens] # bag-of-word(bow) tuples for hver text - (token-id, optælling)\n",
    "\n",
    "print(tweets_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kan tjekke, hvilke ord ligger bag id'erne, med følgende kode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(id2token[id], freq) for id, freq in tweets_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da vi ikke har ændret rækkefølgen, kan vi også finde tweetet i vores dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df.loc[0, 'full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opbygning af LDA model\n",
    "\n",
    "Data er nu klar til at indgå i en gensim LDA model. Modellens parametre kan justeres på forskellig vis, men vi kører her modellem med \"standardindstillingerne\".\n",
    "\n",
    "Modellen skal bruge corpus, dictionary og antal topics (hvis ikke man specificerer topics, kører den med 100 topics - se evt. `?gensim.models.ldamodel.LdaModel` Her køres med 10 topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "lda_model = LdaModel(corpus=tweets_corpus,  # Corpus af tekster\n",
    "                    id2word=id2token,       # Dictionary af ord-id mapping\n",
    "                     num_topics=10,          # Antal topics\n",
    "                     random_state=142)       # Sætter seed - sikrer samme resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Når modellen er kørt, kan vi inspicere, hvilke ord, der er mest sandsynlige inden for hvert topic (vi bruger `pprint()` for at gøre printet lidt mere overskueligt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(lda_model.show_topics(formatted=False)) # Viser de 10 mest sandsynlige ord per topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brug af LDA model\n",
    "\n",
    "Med modellen defineret, kan vi anvende modellen på data igen til at fastslå, hvilke topics er mest fremtrædende i et tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tweets_df.loc[0, 'full_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model[tweets_corpus[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputtet viser - groft sagt - at tweetet består af 87% topic 4 og 6,4% topic 6. Topic 4 er altså det mest dominerende topic i tweetet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fortolkning af LDA modeller\n",
    "\n",
    "LDA modeller kan hjælpe os til at finde umiddelbare temaer. Modellen kan dog ikke fortælle, hvad disse temaer egentlig indfanger. At arbejde med topic models er derfor ofte en kvali-kvantitativ proces, hvor man bruger LDA modellen til at udlede temaer, og derefter kvalitativt forsøger at udelde, hvad disse temaer indfanger. \n",
    "\n",
    "I denne proces finder man ofte også ud af, at visse topics indfanger det samme. På den måde er det også en iterativ proces, hvor man tilpasser modellen til bedre at indfange meningsfulde temaer i teksterne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ØVELSE 5: Topic models i Python (reddit data)\n",
    "\n",
    "I skal nu lave en topic model for reddit kommentarerne.\n",
    "\n",
    "1. Dan et \"gensim corpus\" ud fra reddit kommentarerne (`comment_body`)\n",
    "2. Dan en topic model over kommentarerne (bestem selv antal topics)\n",
    "3. Inspicér de mest sandsynlige ord i hvert topic (`lda_model.show_topics(formatted=False)`)\n",
    "4. Hvad kan I bruge modellen til?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prævalente topics i tekst (supplerende materiale)\n",
    "\n",
    "Vi har set på, hvordan vi kan lave en LDA model til at finde umiddelbare temaer i en tekst. Det, som kunne være interessant at se på, er, hvordan disse topics opstår i data. \n",
    "\n",
    "Dette kan hjælpe os til at blive klogere på, hvad disse topics egentlig indfanger, da vi på den måde kan finde tekster, hvor bestemte topics er meget prævalente.\n",
    "\n",
    "I det følgende laves en funktion, der finder det mest prævalente topic i et tweet, sammen med dens sandsynlighed/\"prævalens-score\" samt nøgleordene for det topic generelt. Disse oplysinger tilføjes derefter som kolonner til datasættet med tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting dominant topic for one corpus entry (bag-of-word tuples - bowt)\n",
    "def get_dominant_topic(text_bowt, ldamodel = lda_model):\n",
    "    \n",
    "    dominant_topic_dict = dict()\n",
    "    \n",
    "    topics_doc = ldamodel[text_bowt]\n",
    "    \n",
    "    dominant_topic = sorted(topics_doc, key = lambda t: t[1], reverse = True)[0]\n",
    "    topic_num = dominant_topic[0]\n",
    "    topic_prob = dominant_topic[1]\n",
    "    \n",
    "    topic_kws = [word for word, prop in ldamodel.show_topic(topic_num)]\n",
    "    \n",
    "    dominant_topic_dict['dominant_topic'] = topic_num\n",
    "    dominant_topic_dict['topic_probability'] = topic_prob\n",
    "    dominant_topic_dict['topic_keywords'] = topic_kws\n",
    "    \n",
    "    return(dominant_topic_dict) # Note that domninant topic info is returned as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of dictionaries - one dictionary contatining dominant topic info for each corpus entry\n",
    "corpus_dominant_topics = list()\n",
    "\n",
    "for tweet_bowt in tweets_corpus:\n",
    "    dominant_topic = get_dominant_topic(tweet_bowt)\n",
    "    corpus_dominant_topics.append(dominant_topic)\n",
    "\n",
    "print(corpus_dominant_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the dominant topic list as a series to the original data frame (NOTE: This assumes that order of entries has not been changed)    \n",
    "tweets_df['dominant_topic_dict'] = pd.Series(corpus_dominant_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spreading dominant topic dictionaries into columns using json_normalize\n",
    "tweets_df_topics = pd.merge(tweets_df, pd.json_normalize(tweets_df['dominant_topic_dict']), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check resultS\n",
    "tweets_df_topics.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
